---
title: "MS_Imp_Guidelines"
output: html_document
date: '2022-07-21'
---

Based on published guidance on computational modelling in health economics and other disciplines as well as our own experience, we have identified 20 standards that we believe are important for implementing a MOSHEM that is accountable (seven standards), reusable (nine standards) and updatable (four standards). The standards are specified in Table \@ref(tab:timelygls) and are discussed below.

```{r, child = child_docs_ls$one, echo=FALSE}
```

### Standards for an accountable MOSHEM
Guidance on transparency in health economic modelling published over ten years ago [@Eddy2012] made recommendations on documenting models but notably did not include recommendations on sharing model code and data. However, more recent and multidisciplinary healthcare modelling guidance [@Erdemir2020] recommends using existing digital repository services to these types of digital model artefacts. Some types of repository such as GitHub [@github2007] provide tools for disseminating work in progress code and providing highly transparent records of the complete development history and individual authorship contributions of a software project, while others such as Zenodo [@Zenodo2013] and Dataverse [@Dataverse2007] provide persistent storage solutions that generate a Digital Object Identifier (DOI) for each code and data collection. 

Model code and data should be clearly documented, potentially with different versions for technical and non-technical users [@Eddy2012]. Consistent use of meaningful naming conventions when authoring code is recommended [@Wilson_2017; @Alarid2019]. Code can be made easier to follow by using the practices of abstraction [@8717448], where only simple, high level commands are routinely exposed to reviewers, and polymorphism [@7181447], where the same command (e.g. "simulate") can be reused to implement different algorithms of the same type. Programs to implement model analyses can be made comprehensible to even non-technical users through the use of literate programming techniques and tools like RMarkdown [@xie2018r] that integrate computer code with plain English descriptions. 

An essential component of quality assuring health economic models is verification - ensuring that calculations are correct and consistent with model specifications [@techver2019]. One useful concept for informing model users about the extensiveness of verification checks is code coverage [@ERICWONG2010188] - the proportion of model code that has been explicitly tested. <!-- In R, the testthat [@testthat2011] and covr [@covr2020] tools can be used in conjunction with GitHub to define tests and report coverage metrics. -->Transcription errors - mistakes introduced when transferring data between sources, models and reports - are very common in health economic models [@Radeva2020]. The risk of these errors might be lower if there was full transparency across all steps in a study workflow. Scientific computing tools now make it relatively straightforward to author programs that reproducibly execute all steps in data ingest, processing and reporting [@Wilson_2017].

Code and data should be distributed with tools that make it easy for potential users to appropriately cite each model artefact. 

### Standards for a reusable MOSHEM
```{r echo = F}
gpl_pc_dbl <- get_license_share("GPL")
```

To make model code and data widely re-usable by others, it is important to provide users with appropriate and explicit permissions. In the context of open source models, there are two broad categories of licensing options. Some guidance strongly recommends the use of permissive licensing [@Wilson_2017] that provides users with great flexibility as to the purposes (including commercial) for which the content could be re-used. An alternative approach is to use copyleft licenses [@copyleft2022] that can require content users to distribute any derivative works they create under similar open source arrangements. For code, it may be appropriate to adopt the prevailing open source licensing practice within the programming language being used. For data, it may not be sufficient to simply choose between a permissive license like the Public Domain Dedication (CC0) [@cc02022] or a copyleft option such as the Attribution-Share Alike (CC-BY-SA) [@bysa2022]. In addition to ensuring that data is ethically appropriate for disseminate in open access repositories, responsible custodianship of some de-identified or aggregated data may involve using or adapting template terms of use [@sampleterms2022] which have a number of ethical clauses (for example, prohibiting efforts to re-identify research participants). 

Storing model code and data in separate files and locations (as opposed to hard coding - embedding data into source code) can make it easier to apply models to different decision contexts and to selectively restrict access to data that are confidential, while disseminating all other model artefacts. Clear distinctions should be made between model modules (code that defines abstract data structures and the algorithms that can be applied to data described by these structures), model datasets (digital information such as parameter values, unit records, etc) and model analyses (code that links model datasets to model modules and specifies the algorithms to apply to data associated with each module).

The software development practice of encapsulation [@8717448] can be used to help ensure that model modules continue to work as intended when they are combined [@ready4oop2022]. In some cases, combining modules may mean new versions of modules have to be created to better account for interaction effects. The concept of inheritance [@8717448] can be used to write code that efficiently achieves this objective as well as to facilitate selective editing of modules when transferring models to different decision contexts [@ready4oop2022]. Writing algorithms as collections of functions (short, self-contained and reusable software routines that each perform a discrete task) is recommended as good practice for scientific computing [@Wilson_2017]. Functions to implement model algorithms can be associated with data structures (also known as a class) via a special type of function called a method. Model modules of a similar type or purpose can be efficiently distributed and documented by bundling them as code libraries. It is good practice to make available test or toy data to demonstrate the use of model algorithms [@Wilson_2017].

Statistical models are a common output of health economic evaluations, but they are often not reported in a format that enables others to confidently and reliably re-use them for out of sample prediction [@Kearns2013]. Open source approaches can help address this by disseminating code artefacts that enable easy and appropriate use of a statistical model to make predictions with new data. However, great care must be exercised when doing so if models are derived from data on human subjects as some software artefacts by default contain a copy of the source dataset. Such dataset copies must therefore be replaced (for example, with synthetic data) and the amended artefact's predictive performance then retested before any public release. Another way to make MOSHEMs easier to use is to develop simple user-interfaces for non-technical users. 

### Standards for an updatable MOSHEM
To avoid MOSHEMs going stale - losing validity and usefulness with time - they should be routinely updated.  Each update of code and data should be uniquely identifiable and retrievable, a goal that can be facilitated by use of version control tools [@Erdemir2020] such as [@git20XX]. 

Potential users of model code and data should be able to easily identify which version is most appropriate to their needs. Using semantic versioning [@semver20xx] conventions can signal the potential importance of an update to users of model code and data. Also informative is to clearly label the type of code hosted in a repository as either:

- "development" (typically the most comprehensive and up to date source code, but potentially not yet sufficiently well documented and tested for potential users to apply to purposes other than testing);

- "production" (code that has been released with a view to it being applied to its stated purposes and therefore typically required to meet defined documentation and testing standards); or

- "archive" (permanent copies of code at key milestones in its development - potentially useful if seeking to use older versions of code to reproduce analyses).

Continuous integration [@CI2017] tools can help verify that each code update passes multiple quality tests. Finally, using deprecation conventions that take an informative and staged approach to retiring old code and data reduces the risk that model revisions have unintended consequences on third party users.  











